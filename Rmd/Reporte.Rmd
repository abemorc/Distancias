
---
title: "Distancia recorrida Incilius spiculatus"
date: "`r Sys.Date()`"
author: "Midgardo"
output:
  rmdformats::robobook:
    highlight: kate
    toc_depth: 2
    
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(rmdformats)
library(DT)
library(kableExtra)
library(sjPlot)
library(here)
library(broom)
library(bestglm)
library(visreg)

#library(dplyr)

## Global options
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	prompt = TRUE
)

htmltools::tagList(rmarkdown::html_dependency_font_awesome())

```

<style>
.fa {
  padding: 20px;
  font-size: 30px;
  width: 50px;
  text-align: right;
  text-decoration: none;
  margin: 5px 2px;
  float: right;
}
</style>
<!-- Add font awesome icons -->
<a href="https://www.youtube.com/channel/UCrZ-pih8cnC5TurbsfUtR1A" class="fa fa-youtube"></a>
<a href="https://www.instagram.com/herpetofauna_fotografia/?r=nametag" class="fa fa-instagram"></a>


```{r include=FALSE}
#funcion para cambiar colores
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

```


```{r include=FALSE}
texto <- "Preparar los datos"
```

<!-- <br> -->
***

# `r colorize(texto, "darkseagreen")`

> 

##  Importar dataframe
Importar datos del archivo excel
```{r}
#importar datos
df1 <- readxl::read_excel(here("Data", "spicu2.xlsx"), sheet = "in")
```

observacion del *raw data* 
```{r}
df1
```

Con la funcion `str()` se puede ver la estructura de un dataframe y la informacion relevante de este
```{r}
str(df1)
```

La base de datos completa  
```{r echo=FALSE}
datatable(df1)
```


Con esto ya sabemos de que forma organizarnos, asi que para un facil acceso a los
nombres de las variables reemplazaremos los espacios por giones, asi evitamos que
salgan comillas al seleccionarlas mediante $

```{r}
vremove <- c(" ")
vreplace <- c("_")

names(df1) <- gsub(vremove, vreplace, names(df1))
names(df1)
```

## Seleccion variables

Ya con los nombres "corregidos" vamos a seleccionar las variables que nos interesan,
en este caso las que nos indicaron 
```{r}
#seleccionar variables que interesan al tronco 
dfdatos <- df1 %>% 
  select(Sitio, Sexo, Distancia24hrs, Distancia_total_reccorida, Temporada,
         NDVI_SMO:Altura_de_arbol)

str(dfdatos)
```

Al importar archivos es comun que no tengan el tipo de dato correcto para poder realizar
operaciones sobre ellos, por lo que es necesario asegurarse de que cada variable tenga el tipo correcto

Tambien crearemos variables dummy para las variables categricas

```{r include=FALSE}
dfdatosori <- dfdatos
```

## Construir variables dummy
```{r}
dfdatos <- dfdatos %>% 
  mutate(Distancia24hrs = as.numeric(Distancia24hrs),
         Sitio_cons = ifelse(Sitio == "Conservado", 1, 0),
         Sexo_macho = ifelse(Sexo == "Macho", 1, 0),
         Sex_hembra = ifelse(Sexo == "Hembra", 1, 0),
         Temporada_seca = ifelse(Temporada == "Secas", 1, 0)) %>% 
  select(Distancia24hrs, Distancia_total_reccorida, NDVI_SMO:Altura_de_arbol,
         Sitio_cons:Temporada_seca)

#verificar que todo este correcto
str(dfdatos)
```

Veamos como quedaron nuestro datos ya preparados para los analisis, para este caso son pocas obsevaciones y pocas 
variables, pero suponiendo que fuesen mas añadiremos unos filtros para poder facilmente
realizar una exploracion de los datos mediante la tabla.
```{r echo=FALSE}
datatable(dfdatos, filter = "bottom")
```

>Se realizara el analisis para la distancia por dia `$Distancia24hrs` y para la distancia
total recorrida `Distancia_total_recorrida`

<!-- aqui empieza lo chido -->


```{r include=FALSE}
texto <- "Analisis distancia por día recorrida"
```
# `r colorize(texto, "darkseagreen")`

Construiremos el dataframe que utilizaremos para este analisis de distancia por dia
```{r}
dfdatos1 <- dfdatos[,c(1, 3:10, 12, 13)]
```

```{r echo=FALSE}
datatable(dfdatos1, style = "bootstrap4")
```

Veamos una primera impresion de la distribucion de los datos
```{r}
hist(x = dfdatos1$Distancia24hrs, breaks = 7, xlab = "Distancia recorrida", main = "Distribucion datos 24hrs")
```

## Estandarización

Para el analisis de correlacion no es necesario estandarizar las variables, sin embargo es comun
hacerlo pues se puede comparar mas facilmente las variables al estar poder medirlas en terminos de 
desviaciones estandar

Crearemos una funcion que a cada valor reste la media de su conjunto y el resultado
lo divida entre la desviacion estandar del mismo

```{r}
Estandarizar <- function(x){
  (x-mean(x, na.rm = T))/sd(x, na.rm = T)
}
```

Y aplicaremos dicha funcion a todas las variables numericas guardandolo en otro datframe,
porque los datos originales los utilizaremos al final
```{r}
dfdatos1Est <- dfdatos1
dfdatos1Est[c(2:8)] <- apply(X = dfdatos1Est[c(2:8)], MARGIN = 2, FUN = Estandarizar)
dfdatos1Est
```

Comprobaremos que su media es igual a cero y su desviación estandar a uno

```{r}
apply(X = dfdatos1Est[c(2:8)], MARGIN = 2, FUN = mean)
apply(X = dfdatos1Est[c(2:8)], MARGIN = 2, FUN = sd)
```

> Con los datos ya estandarizados y las variables dummy creadas podemos porceder a realizar 
los analisis de correlacion

## Correlacion

### Cuantitativas

Nos interesa saber que variables afectan mas a la distancia recorrida asi que podriamos 
realizar un test de correlacion de cada variable en nuestro conjunto de datos respecto a la distancia.
Algo asi:
```{r}
cor(dfdatos1Est$Distancia24hrs, dfdatos1Est$EVI_SMO, use ="complete.obs", method = "spearman")
```

Pero seria mas tardado, asi que haremos una matriz de correlaciones para obtener todas de una vez 
ademas de identificar si algunas de las variables estan relacionadas entre si  

Usaremos el coeficiente de correlacion de spearman ya que nos permite evaluar una
relacion monotona entre dos variables continuas, es decir cuando las variables cambian 
al mismo tiempo pero no necesariamentea un ritmo constante a diferencia del coeficiente de pearson que solo evalua 
una relacion lineal entre las variables.

```{r}
correlaciones <- cor(dfdatos1Est[c(1:8)], use ="complete.obs", method = "spearman")
correlaciones

```

Calcularemos la correlacion en los datos sin estandarizar, solo para comprobar
que los resultados no se ven afectados

```{r}
correlaciones1 <- cor(dfdatos1[c(1:8)], use ="complete.obs", method = "spearman")
correlaciones1
```

Y solo por curiosidad lo haremos mediante el metodo de pearson
```{r}
corrpearson <- cor(dfdatos1Est[c(1:8)], use ="complete.obs", method = "pearson")
corrpearson
```

Con esto vemos que aunque los resultados no son iguales, si son muy similares en cuanto al gardo de correlacion

>

Pongamos bonitos los resultados (Spearman)

```{r}
tab_corr(dfdatos1Est[c(1:8)], corr.method = "spearman")

tab_df(as.data.frame(correlaciones), title = "Correlaciones", 
       footnote = "Calculadas por el metodo de Spearman", show.footnote = T, 
       digits = 3, show.rownames = T)

correlaciones %>%
    kbl(caption = "CORRELACIONES", digits = 3, align = "c") %>%
  kable_classic_2(full_width = F) %>% 
  footnote(general = "Calculado por el método de Spearman")
```
En la tabla podemos observar lo siguiente:  
**NDVI_SMO** y **NDWI_SMO** presentan una relacion inversa muy fuerte.  
**NDVI_SMO** y **NDMI_SMO** presentan una relacion moderada.  
**EVI_SMO** y **LST_SMO** presentan una relacion fuerte.  
**NDWI_SMO** y **NDMI_SMO** presentan una relacion inversa moderada.  

En cuanto a la **Distancia24hrs** (la variable que nos es de interes), 
vemos que **EVI_SMO** y **LST_SMO** serian las que presentan una 
ligera relacion inversa

Ordenemos los resultados de forma descendente en base a su valor absoluto
tomando unicamente la distancia y pongamos los resultados con una bonita 
presentacion
```{r}
relaciones <- as.data.frame(correlaciones)
relaciones[order(relaciones$Distancia24hrs),]
rel <- relaciones[order(abs(relaciones$Distancia24hrs), decreasing = T), 1, drop = F]
rel
```
```{r echo=FALSE}
rel %>%
    kbl(caption = "ORDEN DE CORRELACION RESPECTO A LA DISTANCIA", digits = 3, align = "c") %>%
  kable_classic_2(full_width = F) %>% 
  footnote(general = "Calculado por el método de Spearman")
```


Para estar seguros de los resultados anteriores  calcularemos su grado de significancia

```{r}
probcor <- Hmisc::rcorr(as.matrix(dfdatos1Est[c(1:8)]), type = "spearman")
probcor$P
```

Y tambien los ordenaremos respecto a la distancia
```{r}
probcor1 <- as.data.frame(probcor$P)
probcor1[order(probcor1$Distancia24hrs), 1 , drop = F]
probcor1 <- probcor1[order(probcor1$Distancia24hrs), 1 , drop = F]
probcor1
```

Aquellas que tengan un p_value menor a 0.05 son aquellas que se puede afirmar que 
presentan una relacion 

Pongamos presentables los resultados
```{r echo=FALSE}
probcor$P %>%
    kbl(caption = "SIGNIFICANCIA DE LAS CORRELACIONES", digits = 3, align = "c") %>%
  kable_classic_2(full_width = F) %>% 
  footnote(general = "Calculado por el método de Spearman")

probcor1 %>%
    kbl(caption = "ORDEN DE SIGNIFICANCIA RESPECTO A LA DISTANCIA", digits = 3, align = "c") %>%
  kable_classic_2(full_width = F) %>% 
  footnote(general = "Calculado por el método de Spearman")

```
Aquellas que tengan un p_value menor a 0.05 son aquellas que se puede afirmar que 
presentan una relacion 

Con la informacion de las tablas anterior podemos ver cuales son las variables que tienen mayor
relacion con la distancia recorrida asi como entre ellas

Grafiquemos los resultados obtenidos

```{r}
pairs(dfdatos1Est[c(1:8)])
PerformanceAnalytics::chart.Correlation(dfdatos1Est[c(1:8)], histogram = T,
                                        method = "spearman")
```

Pongamos todo en una sola grafica 
```{r}
psych::pairs.panels(dfdatos1Est[c(1:8)], method = "spearman", stars = TRUE,  
             hist.col = 4, smooth = TRUE, scale = F, density = TRUE,
             pch = 21, lm = F, jiggle = T, ci = TRUE)
```

> Con esto ya tenemos claro cuales variables cuantitativas influyen mas en la distancia

### Categoricas

Veamos visualmente como estan distribuidas las variables categoricas

```{r}
boxplot(formula = as.numeric(Distancia24hrs) ~ Sitio, data = na.omit(dfdatosori),
        na.action=na.pass)
boxplot(formula = as.numeric(Distancia24hrs) ~ Sexo, data = na.omit(dfdatosori),
        na.action=na.pass)
boxplot(formula = as.numeric(Distancia24hrs) ~ Temporada, data = na.omit(dfdatosori),
        na.action=na.pass)
```

Podemos observar que el `Sitio perturbado`, `Sexo_hembra` y `Temporada_lluvias` presentan
mayores distancias que su contraparte

Sin embargo eso no nos dice cual de las tres influya mas en la distanca, para determinarlo
usaremos un metodo llamado point biserial correlation, que nos permite calcular la correlacion
entre una variable categorica y una cuantitativa.
```{r}
relacionSexo <- cor.test(dfdatos1Est$Sex_hembra, dfdatos1Est$Distancia24hrs) 
relacionSitio <- cor.test(dfdatos1Est$Sitio_cons, dfdatos1Est$Distancia24hrs)
relacionTemporada <- cor.test(dfdatos1Est$Temporada_seca, dfdatos1Est$Distancia24hrs)

print(relacionSexo)
print(relacionSitio)
print(relacionTemporada)
```

Con esto comprobamos lo que observamos en los boxplot:  
**Sexo_hembra** tiene relacion positiva fuerte  
**Sitio_cons** tiene relacion inversa debil  
**Temporada_seca** tiene relacion inversa moderada  

Las ordenaremos de acuerdo a su grado de significancia
```{r}
v <- c(relacionSexo$p.value, relacionSitio$p.value, relacionTemporada$p.value)
names(v) <- c("Sexo", "Sitio", "Temporada")
v <- as.data.frame(v[order(v)])
names(v) <- "p_value"
v
```
```{r echo=FALSE}
v %>%
    kbl(caption = "ORDEN DE SIGNIFICANCIA"
        , digits = 3, align = "c") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Point biserial correlation")

```

Graficando los resultados
```{r}
dfdatos1Est1 <- na.omit(dfdatos1Est)
dfdatos1Est1
cor(dfdatos1Est1[9:11])
corrplot::corrplot(cor(dfdatos1Est1[c(1, 9:11)]))

```

Basandonos en el *p_value* haremos una lista con el orden de importancia de
nuestro dataset para la variable `Distancia24hrs` con su respectiva tabla
```{r}
names(v) <- names(probcor1)
str(rbind(v,probcor1))
importancia_variables <- rbind(probcor1, v)
importancia_variables <-  importancia_variables[order(importancia_variables), ,
                                                drop = F]

importancia_variables

```
```{r echo=FALSE}
a <- length(importancia_variables$Distancia24hrs)
importancia_variables[-a, ,drop = F] %>%
    kbl(caption = "ORDEN DE SIGNIFICANCIA RESPECTO A LA DISTANCIA POR DIA"
        , digits = 3, align = "c") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Valores redondeados")

```
> 

## Regresion lineal simple

Se intentara ajustar los datos a un modelo, se iniciara con uno lineal simple
y con los reultados obtenidos determinaremos si es suficiente mente bueno o si es
necesario buscar otro tipo de modelo

Se usara el conjunto de datos con las variables estandarizadas para asi poder ver
directamente en los coeficientes obtenidos su grado de importancia en el modelo
ajustado

En esta fase inicial se utilizara un enfoque de prueba y error para buscar el mejor
modelo

### Modelo 1

El primer intento incluira todas las variables para tener un punto de partida al
intentar mejorar el modelo
```{r}
m1 <- lm(formula = Distancia24hrs ~ ., data = dfdatos1Est1)
summary(m1)
```

Graficar los residuales
```{r}
m1res <- resid(m1)
plot(m1)
qqnorm(m1res)
hist(m1res)
plot(density(m1res))
```

Comprobando mediante test de normalidad
```{r}
olsrr::ols_test_normality(m1)
```

Criterios de desempeño del modelo
```{r}
AIC(m1)
BIC(m1)
```

> La inspeccion visual de los graficos indican que no se ajustan muy bien los datos a un modelo
lineal  
Los resultados de los test de normalidad indican que no se presenta una distribucion normal,
por lo que talvez un modelo lineal no sea lo mas adecuado.  
El modelo no parece ser el mas adecuado, se creara un nuevo modelo lineal con menos
variables seleccionadas al azar para confirmar


### Modelo 2

Tomamos algunas variables seleccionadas al azar
```{r}
m2 <- lm(formula = Distancia24hrs ~ EVI_SMO + LST_SMO + NDWI_SMO, data = dfdatos1Est1)
summary(m2)
m2res <- resid(m2)
plot(density(m2res))
```

Test de normalidad
```{r}
olsrr::ols_test_normality(m2)
```

Criterios de desempeño del modelo
```{r}
AIC(m2)
BIC(m2)
```

> El desempeño empeora respecto al modelo 1

### Modelo 3

Basados en nuestro analisis previo de correlación, tomaremos las variables que
identificamos con mayor relacion a la distancia para este modelo

```{r}
m3 <- lm(formula = Distancia24hrs ~ EVI_SMO + LST_SMO + NDWI_SMO + Sex_hembra +
           Temporada_seca, data = dfdatos1Est1)
summary(m3)
m3res <- resid(m3)
plot(m3)
qqnorm(m3res)
hist(m3res)
plot(density(m3res))

```

Test de normalidad
```{r}
olsrr::ols_test_normality(m3)
```

Criterios de desempeño del modelo
```{r}
AIC(m3)
BIC(m3)
```

> Hay una mejora significativa respecto a los otros dos modelos

### Modelo 4

Seleccionando unicamente las variables identifacdas en el analisis de correlacion
con mayor relevancia

```{r}
m4 <- lm(formula = Distancia24hrs ~ EVI_SMO + LST_SMO + Sex_hembra +
           Temporada_seca, data = dfdatos1Est1)
summary(m4)
m4res <- resid(m4)
plot(m4)
qqnorm(m4res)
hist(m4res)
plot(density(m4res))
```

-Test de normalidad
```{r}
olsrr::ols_test_normality(m4)
```

-Criterios de desempeño del modelo
```{r}
AIC(m4)
BIC(m4)
```

### Conclusiónes

> * Se logró mejorar el modelo inicial, sin embargo se confirma en todos los casos que un modelo lineal simple no es la mejor opción  
* Se podria probar todas las combinaciones posibles para encontrar el modelo lineal mas optimo sin embargo no tiene caso ya que los test indican una no normalidad, por lo que es mejor buscar otras opciones.  
* Despues de inspeccionar los graficos se observa que una distribucion gamma podria ser la mas adecuada para ajustar a estos datos.  
* Se probara mediante un modelo lineal generalizado con una ditribucion gamma y su variante.


## Regresion lineal generalizada (GLM)

Se utilizara un modelo lineal generalizado para una distribucion de probabilidad
de la familia gamma con una funcion de enlace recíproca y la variante logaritmica.

### GLM 1

Se probara primero con todas las variables del dataset 
```{r}
gm1 <- glm(formula = Distancia24hrs ~., family = Gamma, data = dfdatos1Est1)
```

Resumen del glm 1
```{r}
summary(gm1)
gm1res <- resid(gm1)
qqnorm(gm1res)
hist(gm1res)
plot(density(gm1res))
```

Criterios de desempeño
```{r}
AIC(gm1)
BIC(gm1)
#pseudo r2
r2g1 <-  1-gm1$deviance/gm1$null.deviance   
```

Ordenando los resultados
```{r echo=FALSE}
t1 <- tidy(gm1)
tg1 <- glance(gm1)
tg1$pseudoR2 <- r2g1

t1 %>%
    kbl(caption = "GLM GAMMA CON TODAS LAS VARIABLES",
        digits = 3, align = "c", label = "Tabla 1") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Los resultados han sido redondeados a tres cifras")

tg1 %>%
    kbl(caption = "Criterios de desempeño GLM 1",
        digits = 3, align = "c", label = "Tabla 1") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Los resultados han sido redondeados a tres cifras")

```

> No es exactamente el mejor modelo, es necesario modificarlo para obtener mayor 
significancia en los predictores

### GLM´s multiples

El paso siguiente es determinar que variables son las que nos permiten predecir mejor
los resultados, se puede tener el mismo enfoque que se hizo previamente en los modelos
simples pero se ira un paso mas lejos y probaremos con todas las posibles combinaciones
de las variables predictoras para encontrar el mejor modelo posible.

El "mejor" modelo sera determinado usando el criterio de akaike  
```{r}
dfmatrix <- dfdatos1Est1[,c(2:length(dfdatos1Est1),1)]
names(dfmatrix)[11] <- "y"
dfmatrix
dfmatrix <- as.data.frame(dfmatrix)

modelos1 <- bestglm(Xy = dfmatrix, 
                   family = Gamma(link = "inverse"), 
                   IC = "AIC",
                   method = "exhaustive")

```

Los 5 mejores modelos generalizados de una distribucion de probabilidades gamma 
con funcion de enlace reciproca son los siguientes:
```{r}
modelos1$BestModels
```

```{r echo=FALSE}
t1 <- modelos1$BestModels
t1$Modelo <- 1:5


t1[c(12, 1:11)] %>%
    kbl(caption = "Best GLM gamma inverse",
        digits = 3, align = "c", label = "Tabla 1") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(number = c("Determinados por el criterio de akaike 'AIC()'",
                        "Cada fila representa un modelo",
                        "Mediante TRUE o FALSE se indica si esa variable es incluida o no en ese modelo"))
```

Cambiando la funcion de enlace recíproca por una logaritmica y probando todas las 
combinaciones posibles
```{r}
modelos2 <- bestglm(Xy = dfmatrix, 
                    family = Gamma(link = "log"), 
                    IC = "AIC",
                    method = "exhaustive")
```

Los 5 mejores modelos generalizados de una distribucion de probabilidades gamma 
con funcion de enlace logaritmica son los siguientes:
```{r}
modelos2$BestModels
```

```{r echo=FALSE}
t2 <- modelos2$BestModels
t2$Modelo <- 1:5


t2[c(12, 1:10)] %>%
    kbl(caption = "Best GLM gamma log",
        digits = 3, align = "c", label = "Tabla 2") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(number = c("Determinados por el criterio de akaike 'AIC()'",
                        "Cada fila representa un modelo",
                        "Mediante TRUE o FALSE se indica si esa variable es incluida o no en ese modelo"))
```

> Se observa que el mejor modelo posible pertenece a la familialogaritmica, el cual incluye las variables:  
**NDVI_SMO**  
**NDWI_SMO**  
**LST_SMO**  
**Sex_hembra**  
**Temporada_seca**  
Es de resaltar que precisamente estas variables son las que resultaron en el 
analisis de correlación.

## Modelo final

Se hara el modelo obtenido en los resultados anteriores
```{r}

gm3 <- glm(formula = Distancia24hrs ~ NDVI_SMO + NDWI_SMO + LST_SMO + Sex_hembra +
             Temporada_seca, family = Gamma(link = "log"), data = dfdatos1Est1)
summary(gm3)
```

Criterios de desempeño
```{r}
AIC(gm3)
BIC(gm3)
#pseudo r2
r2g3 <-  1-gm3$deviance/gm3$null.deviance   
```

Presentacion de los resultados
```{r echo=FALSE}
t3 <- tidy(gm3)
tg3 <- glance(gm3)
tg3$pseudoR2 <- r2g3

t3 %>%
    kbl(caption = "GLM optimo",
        digits = 3, align = "c", label = "Tabla 2") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Los resultados han sido redondeados a tres cifras")

tg3 %>%
    kbl(caption = "Criterios de desempeño GLM optimo",
        digits = 3, align = "c", label = "Tabla 3") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Los resultados han sido redondeados a tres cifras")
```

Graficas del  modelo
```{r echo=FALSE}
#visreg(gm3, "NDVI_SMO"
#       gg = TRUE, 
#       scale="response")

```

```{r echo=FALSE}
df1 <- na.omit(dfdatos1)
gm9 <- glm(formula = Distancia24hrs ~ NDVI_SMO + NDWI_SMO + LST_SMO + Sex_hembra +
             Temporada_seca, family = Gamma(link = "log"), data = df1)

#plot_model(gm9, type="pred")
#visreg(gm9, "LST_SMO" )

visreg(gm9, 
       gg = T, scale = "response")
```

### Conclusiones

Como se estandarizaron las variables, es posible ver en los coeficientes a cuales
variables nuestro modelo les da mayor importancia

Tomando su valor absoluto el resultado sería el siguiente
```{r echo=FALSE}
tt <- order(abs(t3$estimate), decreasing = T)
tt1 <- t3[tt[2:length(tt)],c(1,2)]
tt1 %>%
    kbl(caption = "Ranking variables del modelo",
        digits = 3, align = "c", label = "Tabla 3") %>%
    kable_classic_2(full_width = F) %>% 
    footnote(general = "Basado en el valor absoluto del coeficiente")
```

Es posible realizar predicciones con el modelo creado  
Para este ejemplo utilizaremos los datos originales, es decir los que estan en el
archivo excel

```{r}
predicciones <- exp(predict(object = gm3, newdata = dfdatos1Est))

```

Como en el conjunto original son 30 observaciones, se obtienen 30 predicciones
```{r}
predicciones
```

Es decir podemos usar nuestro propio modelo para predecir los 3 valores faltantes
de nuestro conjunto de datos original, ya que las demas variables si las teniamos
disponibles 
```{r}
dfdatos1Est$Predicciones <- predicciones
largo <- length(dfdatos1Est)
mostrar <- c(largo, 1:(largo-1))

dfdatos1Est[mostrar]
```
 
```{r echo=FALSE}
dfdatos1Est[mostrar] %>%
    kbl(caption = "Conjunto de datos con predicciones",
        digits = 3, align = "c", label = "Tabla 3") %>%
    kable_classic_2(full_width = F)
```


```{r include=FALSE}
texto <- "Analisis distancia total"
```
# `r colorize(texto, "darkseagreen")`



